{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d5210a7",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34be83c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Model Libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from sklearn.utils import class_weight\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd1e982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File Management Libraries and Helper Functions\n",
    "import os\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import pickle\n",
    "\n",
    "from breed_helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f41bc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Object anda Data Structure Management Libraries\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "37ff50e8",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98d5a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_root = 'data/Images'\n",
    "annot_root = 'data/Annotation'\n",
    "\n",
    "dog_paths = np.array(glob.glob(image_root + '/*/*'))\n",
    "annotations = np.array(glob.glob(annot_root + '/*/*'))\n",
    "breed_list = [x.split('-', 1)[-1] for x in os.listdir(image_root)]\n",
    "\n",
    "for i in range(len(annotations)):\n",
    "        dog_paths[i] = dog_paths[i].replace('\\\\','/')\n",
    "        annotations[i] = annotations[i].replace('\\\\','/')\n",
    "\n",
    "# for i in range(len(breed_list)):\n",
    "#     breed_list[i] = breed_list[i].split('-', 1)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f50fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "        {'Breed': [get_dog_breed(x) for x in annotations], \n",
    "        'Folder_Dir': [x.split('/')[2].split('-')[0] for x in annotations], \n",
    "        'Image_Dir': [x.split('/')[-1] for x in annotations],\n",
    "        'Bbox': [get_bbox(x) for x in annotations],\n",
    "        'Num_Dogs': [len(get_bbox(x)) for x in annotations],\n",
    "        'Image_Path': dog_paths})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6423efea",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(breed_list)\n",
    "print(annotations)\n",
    "print(dog_paths)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "08b0fa24",
   "metadata": {},
   "source": [
    "### Viewing/Expiriments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34e51c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c5c581",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(images)):\n",
    "    if (np.shape(images[i]) != (299, 299, 3)):\n",
    "        print(i)\n",
    "        print(images[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa97d5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "item = df['Image_Path'].iloc[13680]\n",
    "item = Image.open(item).convert('RGB').resize((desired_width, desired_height))\n",
    "np.shape(np.asarray(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5b6dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.stack(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b67b60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_len = list()\n",
    "y_len = list()\n",
    "\n",
    "for bbox_arr in df.Bbox:\n",
    "    for bbox in bbox_arr:\n",
    "        xmin, ymin, xmax, ymax = bbox\n",
    "        x_len.append(xmax-xmin)\n",
    "        y_len.append(ymax-ymin)\n",
    "\n",
    "x_avg = sum(x_len)/len(x_len)\n",
    "x_min = min(x_len)\n",
    "x_max = max(x_len)\n",
    "\n",
    "y_avg = sum(y_len)/len(y_len)\n",
    "y_min = min(y_len)\n",
    "y_max = max(y_len)\n",
    "\n",
    "print(f'x_min: {x_min}, x_avg: {x_avg}, x_max: {x_max}, y_min: {y_min}, y_avg: {y_avg}, y_max: {y_max}')\n",
    "print(sorted(x_len, reverse=True))\n",
    "print(sorted(y_len, reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e3d57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(annotations[0])\n",
    "assert get_image_path(annotations[0]) == dog_paths[0]\n",
    "print(dog_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b10dce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(8):\n",
    "    print(get_bbox(annotations[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b209b02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "for i in range(8):\n",
    "    plt.subplot(2, 4, i+1)\n",
    "\n",
    "    bbox = get_bbox(annotations[i])\n",
    "\n",
    "    plt.axis(\"off\")\n",
    "    dog = get_image_path(annotations[i])\n",
    "    im = Image.open(dog)\n",
    "    #im = im.resize((256,256), Image.ANTIALIAS)\n",
    "    plt.imshow(im)\n",
    "    \n",
    "    for j in range(len(bbox)):\n",
    "        xmin, ymin, xmax, ymax = bbox[j]\n",
    "        plt.plot([xmin, xmax, xmax, xmin, xmin], [ymin, ymin, ymax, ymax, ymin]) # showing border\n",
    "        plt.text(xmin, ymin, get_dog_breed(annotations[i]), bbox={'ec': None})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e330d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_cropped():\n",
    "#     #plt.figure(figsize=(10,6))\n",
    "#     for i in range(len(dog_image_paths)):\n",
    "#         bbox = get_bbox(annotations[i])\n",
    "#         dog = get_image_path(annotations[i])\n",
    "#         im = Image.open(dog)\n",
    "#         for j in range(len(bbox)):\n",
    "#             im2 = im.crop(bbox[j])\n",
    "#             #im2 = im2.resize((331,331), Image.ANTIALIAS)\n",
    "#             new_path = dog.replace('data/Images/','data/Cropped/')\n",
    "#             new_path = new_path.replace('.jpg', '-' + str(j) + '.jpg')\n",
    "#             im2 = im2.convert('RGB')\n",
    "#             head, tail = os.path.split(new_path)\n",
    "#             Path(head).mkdir(parents=True, exist_ok=True)\n",
    "#             im2.save(new_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993f7c3a",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ed0ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/shuffled_brute_resized_299_images.pickle', 'rb') as file:\n",
    "    X, y = pickle.load(file).values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50cd81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'X size:{np.shape(X)}, y size:{np.shape(y)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7145e362",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, stratify = y, random_state = 42)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size = 0.5, stratify = y_test, random_state = 1)\n",
    "print(\"X train, test, val: \", len(X_train), len(X_test), len(X_val))\n",
    "print(\"y train, test, val: \", len(y_train), len(y_test), len(y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0c9a51",
   "metadata": {},
   "source": [
    "### Model Defintion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461f7089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class lr_scheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "#     def __init__(self, d_model, warmup_steps=4000, ramp_scalar=1, decay_scalar=1):\n",
    "#         super().__init__()\n",
    "\n",
    "#         self.d_model = d_model\n",
    "#         self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "#         self.ramp_scalar = ramp_scalar\n",
    "#         self.decay_scalar = decay_scalar\n",
    "        \n",
    "#         self.warmup_steps = warmup_steps\n",
    "\n",
    "#     def __call__(self, step):\n",
    "#         step = tf.cast(step, dtype=tf.float32)\n",
    "#         arg1 = tf.math.rsqrt(step) * self.decay_scalar\n",
    "#         arg2 = step * (self.warmup_steps ** -1.5) * self.ramp_scalar\n",
    "#         return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "    \n",
    "#     def get_config(self):\n",
    "#         return {\n",
    "#             \"d_model\": self.d_model,\n",
    "#             \"warmup_steps\": self.warmup_steps,\n",
    "#         }\n",
    "    \n",
    "#     def from_config(cls, config):\n",
    "#          return cls(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f71a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_augmentation(input, severity):\n",
    "    x = layers.RandomFlip(\"horizontal\")(input)\n",
    "    x = layers.RandomTranslation(severity, severity)(x)\n",
    "    x = layers.RandomZoom(severity)(x)\n",
    "    x = layers.RandomRotation(2*severity)(x)\n",
    "    x = layers.RandomContrast(severity)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ddb815",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_learning(input, cnn_modules=3, starting_features=32, feature_ramp=2):\n",
    "    features = starting_features\n",
    "\n",
    "    x = layers.Conv2D(starting_features, kernel_size=5, activation='relu')(input)\n",
    "    x = layers.MaxPooling2D(3, strides=2)(x)\n",
    "\n",
    "    for i in range(cnn_modules-1):\n",
    "        features *= feature_ramp\n",
    "        x = layers.Conv2D(features, kernel_size=5, activation='relu')(x)\n",
    "        x = layers.MaxPooling2D(2, strides=1)(x)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99cff56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def cnn_classifier_bridge(output_size):\n",
    "#     x = layers.Flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3136202d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_classifier(input, num_classes, dff=1024, dff_decay=0.5, dropout=0.1):\n",
    "    counter = dff\n",
    "    x = layers.Flatten()(input)\n",
    "\n",
    "    x = layers.Dense(dff, activation='relu')(x)\n",
    "    x = layers.Dropout(dropout)\n",
    "\n",
    "    while counter > num_classes:\n",
    "        counter *= dff_decay\n",
    "        x = layers.Dense(counter, activation='relu')(x)\n",
    "        x = layers.Dropout(dropout)(x)\n",
    "\n",
    "    x = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a82a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_compiled_model(input_shape):    \n",
    "        learning_rate = 0.001\n",
    "\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-7, amsgrad = False)\n",
    "\n",
    "        input = layers.Input(input_shape)\n",
    "        data_aug = data_augmentation(input, severity)\n",
    "        feature_extract = feature_learning(data_aug, cnn_modules, starting_features, feature_ramp)\n",
    "        classifier = dense_classifier(feature_extract, num_classes, dff, dff_decay, dropout_rate)\n",
    "\n",
    "        model = classifier\n",
    "\n",
    "        model.compile(\n",
    "                loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "                optimizer=optimizer,\n",
    "                metrics=['accuracy'])\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505c3ba8",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0282b085",
   "metadata": {},
   "outputs": [],
   "source": [
    "severity = 0.1\n",
    "\n",
    "cnn_modules = 4\n",
    "starting_features = 32\n",
    "feature_ramp = 2\n",
    "\n",
    "dff = 1024\n",
    "dff_decay = 0.5\n",
    "dropout_rate = 0.2\n",
    "\n",
    "num_classes = len(np.unique(y_train))\n",
    "batch_size = 64\n",
    "epochs = 50\n",
    "class_weights = class_weight.compute_class_weight(class_weight='balanced',\n",
    "                                                  classes=np.unique(y_train),\n",
    "                                                  y=y_train) \n",
    "class_weights=dict(zip(np.unique(y_train), class_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4a293d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(batch_size)\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val)).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0dc13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_compiled_model((299, 299, 3))\n",
    "model.build((16464, 299, 299, 3))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110093",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_dataset, batch_size=batch_size, epochs=epochs, \n",
    "                        validation_data=val_dataset)#, \n",
    "                        #class_weight=class_weights, verbose=1, callbacks = [my_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4908aaa0",
   "metadata": {},
   "source": [
    "### Training Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237557ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e51d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "#plt.axis([75, 200, 1.2, 2.2])\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9621343a",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = lr_scheduler(d_model)\n",
    "lr = np.array([])\n",
    "for i in range(1, 100):\n",
    "    lr = np.append(lr, learning_rate.__call__(300*i))\n",
    "plt.title('Learning Rate')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.plot(lr, label='Learning Rate')\n",
    "#plt.axis([0, 300, 0, 0.000125])\n",
    "plt.legend()\n",
    "\n",
    "# learning_rate = lr_scheduler(d_model, ramp_scalar=1, decay_scalar=1)\n",
    "# steps_per_epoch = np.ceil((np.shape(X_train)[0]/batch_size))\n",
    "# print(steps_per_epoch)\n",
    "# lr = np.array([])\n",
    "# for i in range(1, np.maximum(epochs+10, 30)):\n",
    "#     lr = np.append(lr, learning_rate.__call__((i*steps_per_epoch)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9b9282",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbc1bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975b36fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_cm(cm):\n",
    "    return np.array([cm[i] / np.sum(cm[i]) for i in range(len(cm))])\n",
    "def accuracy(cm):\n",
    "    return cm.diagonal().sum() / cm.sum()\n",
    "cm = confusion_matrix(y_test, np.argmax(pred, axis=1))\n",
    "\n",
    "normalized_cm = norm_cm(cm)\n",
    "print('accuracy', accuracy(cm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f613a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(cm, xticklabels=range(2, 11), yticklabels=range(2, 11), annot=True, fmt='g', square=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc99b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(normalized_cm, xticklabels=range(2, 11), yticklabels=range(2, 11), annot=True, fmt='.2f', square=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
